{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Lambda, Flatten, Dense\n",
    "import csv\n",
    "import os\n",
    "import cv2\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D \n",
    "from keras.layers import Cropping2D, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data pointd from \"Driving_Log.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "with open('../data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the training dataset into 80% train and 20% validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read image and convert the image to RGB from BGR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(source_path):\n",
    "    filename = source_path.split('\\\\')[-1]\n",
    "    img_path = '../data/IMG/' + filename\n",
    "    img = cv2.imread(img_path)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment the images by adding flipped image and negate the steering angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_images(car_images, steering_angles):\n",
    "    #using data augmentation to create more training dataset by flipping images\n",
    "    augmented_images, augmented_measurements = [], []\n",
    "    for image, measurement in zip(car_images, steering_angles):\n",
    "        augmented_images.append(image)\n",
    "        augmented_measurements.append(measurement)\n",
    "        augmented_images.append(cv2.flip(image, 1))\n",
    "        augmented_measurements.append(measurement*-1.0)\n",
    "    return [augmented_images, augmented_measurements]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Generator to yield images using a given batch_size\n",
    "1. Used correction to adjust the steering angle for the left and right camera images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "    while 1:\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "            \n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                steering_center = float(batch_sample[3])\n",
    "        \n",
    "                #create adjusted steering measurements for the side camera images\n",
    "                correction = 0.2\n",
    "                steering_left = steering_center + correction\n",
    "                steering_right = steering_center - correction\n",
    "\n",
    "                #read in images from center, left, right cameras\n",
    "                img_center = process_image(batch_sample[0])\n",
    "                img_left = process_image(batch_sample[1])\n",
    "                img_right = process_image(batch_sample[2])\n",
    "\n",
    "                images.extend([img_center, img_left, img_right])\n",
    "                angles.extend([steering_center, steering_left, steering_right])\n",
    "            \n",
    "            augmented_output = augmented_images(images, angles)\n",
    "            X_train = np.array(augmented_output[0])\n",
    "            y_train = np.array(augmented_output[1])\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train the model  using generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and train the model  using generator function\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Nvidia Model from the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rajkuma\\appdata\\local\\continuum\\miniconda3\\envs\\tf35\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (5, 5), activation=\"relu\", strides=(2, 2))`\n",
      "  \"\"\"\n",
      "c:\\users\\rajkuma\\appdata\\local\\continuum\\miniconda3\\envs\\tf35\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(36, (5, 5), activation=\"relu\", strides=(2, 2))`\n",
      "  \n",
      "c:\\users\\rajkuma\\appdata\\local\\continuum\\miniconda3\\envs\\tf35\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (5, 5), activation=\"relu\", strides=(2, 2))`\n",
      "  import sys\n",
      "c:\\users\\rajkuma\\appdata\\local\\continuum\\miniconda3\\envs\\tf35\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  \n",
      "c:\\users\\rajkuma\\appdata\\local\\continuum\\miniconda3\\envs\\tf35\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\")`\n",
      "  if __name__ == '__main__':\n",
      "c:\\users\\rajkuma\\appdata\\local\\continuum\\miniconda3\\envs\\tf35\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "c:\\users\\rajkuma\\appdata\\local\\continuum\\miniconda3\\envs\\tf35\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., validation_data=<generator..., validation_steps=3703, epochs=3, steps_per_epoch=14812)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "14811/14812 [============================>.] - ETA: 0s - loss: 0.0243"
     ]
    }
   ],
   "source": [
    "#Using Nvidia Model\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: (x / 255.0) - 0.5, input_shape=(160, 320, 3)))\n",
    "model.add(Cropping2D(cropping=((70, 25), (0, 0))))\n",
    "model.add(Convolution2D(24,5,5, subsample=(2,2), activation=\"relu\"))\n",
    "model.add(Convolution2D(36,5,5, subsample=(2,2), activation=\"relu\"))\n",
    "model.add(Convolution2D(48,5,5, subsample=(2,2), activation=\"relu\"))\n",
    "model.add(Convolution2D(64,3,3,  activation=\"relu\"))\n",
    "model.add(Convolution2D(64,3,3,  activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100))\n",
    "model.add(Dense(50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit_generator(train_generator, samples_per_epoch=len(train_samples), validation_data=validation_generator, nb_val_samples=len(validation_samples), nb_epoch=3)\n",
    "model.save('model_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 160, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 65, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 31, 158, 24)       1824      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 77, 36)        21636     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 5, 37, 48)         43248     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 35, 64)         27712     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 33, 64)         36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2112)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               211300    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 348,219\n",
      "Trainable params: 348,219\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
